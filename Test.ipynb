{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신종 코로나 바이러스 감염증 ( 코로나 19 ) 사태 가 심각 합니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyPreTokenizer\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "sent_ko = '신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'\n",
    "mecab_pretok = KoNLPyPreTokenizer(Mecab())\n",
    "print(mecab_pretok(sent_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyT5WordPieceTrainer, KoNLPyWordPieceTokenizer\n",
    "from huggingface_konlpy.transformers_konlpy import  KoNLPyT5Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialize alphabet 1/1: 100%|██████████| 70964/70964 [00:00<00:00, 100311.70it/s]\n",
      "Train vocab 1/1: 100%|██████████| 70964/70964 [00:12<00:00, 5460.71it/s]\n"
     ]
    }
   ],
   "source": [
    "t5_mecab_wordpiece_notag_trainer = KoNLPyT5WordPieceTrainer(\n",
    "    Mecab(), use_tag=False)\n",
    "t5_mecab_wordpiece_notag_trainer.train(\n",
    "    files = ['./data/2020-07-29_covid_news_sents.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/home/jovyan/work/huggingface_konlpy/tokenizers/t5-no-tag-vocab.txt]\n"
     ]
    }
   ],
   "source": [
    "t5_mecab_wordpiece_notag_trainer.save_model('./tokenizers/', 't5-no-tag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_t5_notag =  KoNLPyT5Tokenizer(\n",
    "    konlpy_wordpiece = KoNLPyWordPieceTokenizer(Mecab(), use_tag=False),\n",
    "    vocab_file = './tokenizers/t5-no-tag-vocab.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1103, 1023, 1098, 1109, 1016, 1063, 1024, 1014, 1219, 1011, 2112, 1668]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_t5_notag.encode(sent_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_t5_notag.decode([1103, 1023, 1098, 1109, 1016, 1063, 1024, 1014, 1219, 1011, 2112, 1668])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialize alphabet 1/1: 100%|██████████| 70964/70964 [00:00<00:00, 100909.85it/s]\n",
      "Train vocab 1/1: 100%|██████████| 70964/70964 [00:13<00:00, 5222.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/home/jovyan/work/huggingface_konlpy/tokenizers/BertStyleMecab/usetag-vocab.txt]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyBertWordPieceTrainer, KoNLPyWordPieceTokenizer\n",
    "from huggingface_konlpy.transformers_konlpy import  KoNLPyBertTokenizer\n",
    "\n",
    "\n",
    "mecab_wordpiece_usetag_trainer = KoNLPyBertWordPieceTrainer(\n",
    "    Mecab(), use_tag=True)\n",
    "mecab_wordpiece_usetag_trainer.train(\n",
    "    files = ['./data/2020-07-29_covid_news_sents.txt'])\n",
    "mecab_wordpiece_usetag_trainer.save_model('./tokenizers/BertStyleMecab/', 'usetag')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['신종/NNG', '코로나/NNG', '##바이러스/NNG', '감염증/NNG', '##(/SSO', '##코로나/NNP', '##19/SN', '##)/SSC', '사태/NNG', '##가/JKS', '심각/XR', '합', '니', '다']\n"
     ]
    }
   ],
   "source": [
    "mecab_bert_usetag =  KoNLPyBertTokenizer(\n",
    "    konlpy_wordpiece = KoNLPyWordPieceTokenizer(Mecab(), use_tag=True),\n",
    "    vocab_file = './tokenizers/BertStyleMecab/usetag-vocab.txt'\n",
    ")\n",
    "print(mecab_bert_usetag.tokenize(sent_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialize alphabet 1/1: 100%|██████████| 70964/70964 [00:00<00:00, 99327.07it/s]\n",
      "Train vocab 1/1: 100%|██████████| 70964/70964 [00:13<00:00, 5291.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/home/jovyan/work/huggingface_konlpy/tokenizers/BertStyleMecab/notag-vocab.txt]\n",
      "['신종', '코로나', '##바이러스', '감염증', '##(', '##코로나', '##19', '##)', '사태', '##가', '심각', '##합니다']\n"
     ]
    }
   ],
   "source": [
    "mecab_wordpiece_notag_trainer = KoNLPyBertWordPieceTrainer(\n",
    "    Mecab(), use_tag=False)\n",
    "mecab_wordpiece_notag_trainer.train(\n",
    "    files = ['./data/2020-07-29_covid_news_sents.txt'])\n",
    "mecab_wordpiece_notag_trainer.save_model('./tokenizers/BertStyleMecab/', 'notag')\n",
    "\n",
    "mecab_bert_notag = KoNLPyBertTokenizer(\n",
    "    konlpy_wordpiece = KoNLPyWordPieceTokenizer(Mecab(), use_tag=False),\n",
    "    vocab_file = './tokenizers/BertStyleMecab/notag-vocab.txt'\n",
    ")\n",
    "print(mecab_bert_notag.tokenize(sent_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1103, 1023, 1098, 1109, 1016, 1063, 1024, 1014, 1219, 1011, 2112, 1668, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_bert_notag.encode(sent_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Base class for all vocabularies.\"\"\"\n",
    "\n",
    "    def __init__(self, extra_ids=0):\n",
    "        self._extra_ids = extra_ids\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def encode(self, s):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decode(self, ids):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def encode_tf(self, s):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decode_tf(self, ids):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def extra_ids(self):\n",
    "        return self._extra_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KorVocabulary(Vocabulary):\n",
    "    \n",
    "    def __init__(self, vocab_file, extra_ids=None):\n",
    "        \n",
    "        self._vocab_file = vocab_file\n",
    "        self._tokenizer = None\n",
    "        \n",
    "\n",
    "        kwargs = {\"extra_ids\": extra_ids} if extra_ids is not None else {}\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        if not self._tokenizer:\n",
    "            mecab_t5_notag =  KoNLPyT5Tokenizer(\n",
    "                konlpy_wordpiece = KoNLPyWordPieceTokenizer(Mecab(), use_tag=False),\n",
    "                vocab_file = self._vocab_file\n",
    "            )\n",
    "\n",
    "            self._tokenizer = mecab_t5_notag\n",
    "        return self._tokenizer\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "    \n",
    "    def encode(self, s):\n",
    "        return self.tokenizer.encode(s)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode(ids)\n",
    "    \n",
    "    def encode_tf(self, s):\n",
    "        ids = self.encode(s)\n",
    "        return tf.convert_to_tensor(ids, dtype=tf.int32)\n",
    "    def decode_tf(self, ids):\n",
    "        return tf.py_function(func=self.decode, inp=[ids], Tout=tf.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = KorVocabulary(vocab_file='./tokenizers/t5-no-tag-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1103, 1023, 1098, 1109, 1016, 1063, 1024, 1014, 1219, 1011, 2112, 1668]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.encode(sent_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.decode([1103, 1023, 1098, 1109, 1016, 1063, 1024, 1014, 1219, 1011, 2112, 1668])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12,), dtype=int32, numpy=\n",
       "array([1103, 1023, 1098, 1109, 1016, 1063, 1024, 1014, 1219, 1011, 2112,\n",
       "       1668], dtype=int32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.encode_tf(sent_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.decode_tf([1103, 1023, 1098, 1109, 1016, 1063, 1024, 1014, 1219, 1011, 2112, 1668]).numpy().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyPretokBertWordPieceTokenizer\n",
    "from huggingface_konlpy.transformers_konlpy import KoNLPyPretokBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizers/MecabBertWordPieceTokenizer/covid-vocab.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_bertwordpiece_tokenizer = KoNLPyPretokBertWordPieceTokenizer(\n",
    "    konlpy_pretok = mecab_pretok)\n",
    "\n",
    "mecab_bertwordpiece_tokenizer.train(\n",
    "    files = ['./data/2020-07-29_covid_news_sents.txt'],\n",
    "    vocab_size = 3000)\n",
    "mecab_bertwordpiece_tokenizer.save_model(\n",
    "    directory='./tokenizers/MecabBertWordPieceTokenizer/',\n",
    "    name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 신종 코로나 바이러스 감염증 ( 코로나 19 ) 사태 가 심 ##각 합 ##니다 [SEP]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_konlpy import compose\n",
    "from huggingface_konlpy.transformers_konlpy import KoNLPyPretokBertTokenizer\n",
    "\n",
    "mecab_pretok_berttokenizer = KoNLPyPretokBertTokenizer(\n",
    "    konlpy_pretok = mecab_pretok,\n",
    "    vocab_file = './tokenizers/MecabBertWordPieceTokenizer/covid-vocab.txt')\n",
    "\n",
    "indices = mecab_pretok_berttokenizer.encode(sent_ko)\n",
    "tokens = [mecab_pretok_berttokenizer.ids_to_tokens[ids] for ids in indices]\n",
    "print(' '.join(compose(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
